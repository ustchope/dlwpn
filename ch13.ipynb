{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "administrative-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.41 ms (started: 2021-08-07 23:18:30 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 ch13 #1 change Aug 08, 2021'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.64 s (started: 2021-08-07 23:19:44 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-financing",
   "metadata": {},
   "source": [
    "# 现实世界的最佳实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-lodging",
   "metadata": {},
   "source": [
    "**本章内容包含** \n",
    "\n",
    "* 超参数调优\n",
    "* 模型集成\n",
    "* 混合精度训练\n",
    "* 在多个 GPU 或 TPU 上训练 Keras 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-valentine",
   "metadata": {},
   "source": [
    "从这本书开始，你已经走了很远。您现在可以训练图像分类模型、图像分割模型、矢量数据分类或回归模型、时间序列预测模型、文本分类模型、序列到序列模型，甚至文本和图像的生成模型。你已经涵盖了所有的基础。\n",
    "\n",
    "但是，到目前为止，您的模型都经过小规模训练——使用单个 GPU 在小数据集上进行——并且它们通常还没有在我们查看的每个数据集上达到可实现的最佳性能。毕竟，这本书是一本入门书。如果要走出现实世界并在全新问题上取得最先进的结果，您仍然需要跨越一些鸿沟。\n",
    "\n",
    "倒数第二章是关于弥合这一差距，并为您提供从机器学习学生到成熟的机器学习工程师所需的最佳实践。我们将回顾系统地提高模型性能的基本技术：超参数调整和模型集成。然后，我们将研究如何通过多 GPU 和 TPU 训练、混合精度以及利用云中的远程计算资源来加速和扩大模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-welsh",
   "metadata": {},
   "source": [
    "## 充分利用您的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-eight",
   "metadata": {},
   "source": [
    "如果你只需要一些可以正常工作的东西，盲目地尝试不同的架构配置就足够了。 在本节中，我们将超越“工作正常”，到“工作出色并赢得机器学习竞赛”，通过快速指南，了解一组用于构建最先进深度学习的必备技术 楷模。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-leather",
   "metadata": {},
   "source": [
    "### 超参数优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-manitoba",
   "metadata": {},
   "source": [
    "在构建深度学习模型时，您必须做出许多看似随意的决定：您应该堆叠多少层？ 每层应该有多少个单元或过滤器？ 你应该使用 relu 作为激活，还是不同的功能？ 您应该在给定层之后使用 BatchNormalization 吗？ 你应该使用多少dropout？ 等等。 这些架构级参数称为超参数，以将它们与模型的参数区分开来，这些参数是通过反向传播训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-strike",
   "metadata": {},
   "source": [
    "在实践中，经验丰富的机器学习工程师和研究人员会随着时间的推移对这些选择中哪些有效哪些无效建立直觉——他们开发了超参数调整技能。 但是没有正式的规则。 如果您想达到在给定任务上所能达到的极限，您不能满足于这样任意的选择。 即使您有很好的直觉，您最初的决定也几乎总是次优的。 你可以通过手动调整和反复重新训练模型来完善你的选择——这是机器学习工程师和研究人员花费大部分时间做的事情。 但是，作为人类，整天摆弄超参数不应该是你的工作——最好让机器来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-moral",
   "metadata": {},
   "source": [
    "因此，您需要以有原则的方式自动、系统地探索可能的决策空间。 您需要搜索架构空间并根据经验找到性能最佳的架构。 这就是自动超参数优化领域的意义所在：它是一个完整的研究领域，也是一个重要的领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-liabilities",
   "metadata": {},
   "source": [
    "优化超参数的过程通常如下所示：\n",
    "* 选择一组超参数（自动）。\n",
    "* 建立相应的模型。\n",
    "* 将其拟合到您的训练数据，并测量验证数据的性能。\n",
    "* 选择下一组要尝试的超参数（自动）。\n",
    "* 重复。\n",
    "* 最后，测量测试数据的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-halifax",
   "metadata": {},
   "source": [
    "这个过程的关键是分析验证性能和各种超参数值之间关系的算法，以选择下一组要评估的超参数。 许多不同的技术都是可能的：贝叶斯优化、遗传算法、简单随机搜索等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-express",
   "metadata": {},
   "source": [
    "训练模型的权重相对容易：您在小批量数据上计算损失函数，然后使用反向传播将权重向正确的方向移动。另一方面，更新超参数提出了独特的挑战。考虑一下：\n",
    "* 超参数空间通常由离散决策组成，因此不是连续或可微的。因此，您通常无法在超参数空间中进行梯度下降。相反，您必须依赖无梯度优化技术，其效率自然远低于梯度下降。\n",
    "* 计算这个优化过程的反馈信号（这组超参数是否会导致这个任务的高性能模型？）可能非常昂贵：它需要在数据集上从头开始创建和训练一个新模型。\n",
    "* 反馈信号可能是嘈杂的：如果训练运行的性能提高 0.2%，这是因为更好的模型配置，还是因为您对初始权重值很幸运？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-senator",
   "metadata": {},
   "source": [
    "幸运的是，有一个工具可以让超参数调整变得更简单：KerasTuner。 让我们来看看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-publication",
   "metadata": {},
   "source": [
    "**使用 KERASTUNER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-disease",
   "metadata": {},
   "source": [
    "让我们从安装 KerasTuner 开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sized-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -f-nightly-gpu (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -f-nightly-gpu (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Collecting git+https://github.com/keras-team/keras-tuner.git\n",
      "  Cloning https://github.com/keras-team/keras-tuner.git to /tmp/pip-req-build-bv6rpfep\n",
      "  Running command git clone -q https://github.com/keras-team/keras-tuner.git /tmp/pip-req-build-bv6rpfep\n",
      "Requirement already satisfied: packaging in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (20.9)\n",
      "Requirement already satisfied: numpy in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (1.19.5)\n",
      "Requirement already satisfied: requests in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (2.25.1)\n",
      "Requirement already satisfied: scipy in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (1.6.2)\n",
      "Requirement already satisfied: tensorboard in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (2.5.0)\n",
      "Requirement already satisfied: ipython in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (7.23.1)\n",
      "Requirement already satisfied: kt-legacy in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from keras-tuner==1.0.3) (1.0.3)\n",
      "Requirement already satisfied: pickleshare in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (0.7.5)\n",
      "Requirement already satisfied: backcall in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (0.1.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (3.0.18)\n",
      "Requirement already satisfied: setuptools>=18.5 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (57.0.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (5.0.5)\n",
      "Requirement already satisfied: pygments in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (2.9.0)\n",
      "Requirement already satisfied: decorator in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from ipython->keras-tuner==1.0.3) (0.18.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from jedi>=0.16->ipython->keras-tuner==1.0.3) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from pexpect>4.3->ipython->keras-tuner==1.0.3) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner==1.0.3) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from traitlets>=4.2->ipython->keras-tuner==1.0.3) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from packaging->keras-tuner==1.0.3) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from requests->keras-tuner==1.0.3) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from requests->keras-tuner==1.0.3) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from requests->keras-tuner==1.0.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from requests->keras-tuner==1.0.3) (3.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (3.3.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (1.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (3.14.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from tensorboard->keras-tuner==1.0.3) (0.13.0)\n",
      "Requirement already satisfied: six in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard->keras-tuner==1.0.3) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (3.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -f-nightly-gpu (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -f-nightly-gpu (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -f-nightly-gpu (/public/huangwei/miniconda3/envs/tensorflow/lib/python3.8/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "time: 13.1 s (started: 2021-08-08 00:12:43 +08:00)\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/keras-team/keras-tuner.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-georgia",
   "metadata": {},
   "source": [
    "KerasTuner 的核心思想是让你用一系列可能的选择来替换硬编码的超参数值，如units=32，如 Int(name=\"units\", min_value=16, max_value=64, step=16）。 给定模型中的一组此类选择称为超参数调整过程的搜索空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-aviation",
   "metadata": {},
   "source": [
    "要指定搜索空间，请定义模型构建函数（参见代码清单 13.1）。 它接受一个 hp 参数，您可以从中采样超参数范围，并返回一个编译的 Keras 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-daniel",
   "metadata": {},
   "source": [
    "> 清单 13.1 KerasTuner 模型构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "permanent-shareware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.8 ms (started: 2021-08-07 23:35:40 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "#     来自 hp 对象的样本超参数值。 采样后，这些值（例如这里的“单位”变量）只是常规的 Python 常量。\n",
    "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16) \n",
    "    model = keras.Sequential([layers.Dense(units, activation=\"relu\"),\n",
    "                              layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "#     可以使用不同种类的超参数：Int、Float、Boolean、Choice。\n",
    "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"]) \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    # 该函数返回一个编译模型。\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-processing",
   "metadata": {},
   "source": [
    "如果你想采用更加模块化和可配置的模型构建方法，你也可以继承 HyperModel 类并定义一个build()方法，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-might",
   "metadata": {},
   "source": [
    "> 清单 13.2 KerasTuner 超模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unavailable-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.03 ms (started: 2021-08-08 00:13:59 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "class SimpleMLP(kt.HyperModel):\n",
    "    def __init__(self, num_classes): \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "#     该方法与我们之前构建的 build_model 独立函数相同。\n",
    "    def build(self, hp): \n",
    "        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
    "        \n",
    "        # 由于面向对象的方法，我们可以将模型常量配置为构造函数参数（而不是在模型构建函数中对其进行硬编码）。\n",
    "        model = keras.Sequential([layers.Dense(units, activation=\"relu\"),\n",
    "                                  layers.Dense(self.num_classes, activation=\"softmax\") ])\n",
    "        \n",
    "        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "hypermodel = SimpleMLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-memory",
   "metadata": {},
   "source": [
    "下一步是定义一个“调谐器”。 从原理上讲，您可以将调谐器视为循环，它会重复执行：\n",
    "* 选择一组超参数值。\n",
    "* 使用这些值调用模型构建函数以创建模型。\n",
    "* 训练模型并记录其指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-amateur",
   "metadata": {},
   "source": [
    "KerasTuner 有几个可用的内置调谐器 - RandomSearch，BayesianOptimization ，Hyperband 。 让我们尝试一下 BayesianOptimization，这是一个调谐器，它试图做出智能预测，根据先前选择的结果，新的超参数值可能表现最佳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "literary-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.37 s (started: 2021-08-08 00:23:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "#     指定模型构建函数（或超模型实例）\n",
    "    build_model, \n",
    "    # 指定调谐器将寻求优化的指标。 始终指定验证指标，因为搜索过程的目标是找到泛化模型！\n",
    "    objective=\"val_accuracy\", \n",
    "    # 在结束搜索之前要尝试的不同模型配置（“试验”）的最大数量。\n",
    "    max_trials=100, \n",
    "    # 为了减少指标差异，您可以多次训练同一个模型并平均结果。 \n",
    "#     executions_per_trial 是每个模型配置（试验）运行的训练轮次（执行）的数量。\n",
    "    executions_per_trial=2, \n",
    "#   搜索日志的存储位置。\n",
    "    directory=\"mnist_kt_test\", \n",
    "    # 是否覆盖数据以开始新的搜索。 如果您修改了模型构建功能的目录，\n",
    "#     则将此设置为 True，或者将其设置为 False 以使用相同的模型构建功能恢复先前开始的搜索。\n",
    "    overwrite=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-apparatus",
   "metadata": {},
   "source": [
    "您可以通过 search_space_summary() 显示搜索空间的概览："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescription-digest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
      "optimizer (Choice)\n",
      "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n",
      "time: 834 µs (started: 2021-08-08 00:24:44 +08:00)\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surprised-scottish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BayesianOptimization',\n",
       " 'Hyperband',\n",
       " 'RandomSearch',\n",
       " 'Sklearn',\n",
       " 'SklearnTuner',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'absolute_import',\n",
       " 'bayesian',\n",
       " 'hyperband',\n",
       " 'randomsearch',\n",
       " 'sklearn_tuner']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.59 ms (started: 2021-08-08 00:31:44 +08:00)\n"
     ]
    }
   ],
   "source": [
    "dir(kt.tuners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-eugene",
   "metadata": {},
   "source": [
    "> **目标最大化和最小化**\n",
    ">\n",
    "> 对于内置指标（如在我们的例子中的准确性），度量方向的 的（应该最大化准确性，但应该最小化损失）由 KerasTuner 推断。 但是，对于自定义指标，您应该自己指定它，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = kt.Objective(\n",
    "#     指标的名称，在周期日志中找到\n",
    "    name=\"val_accuracy\", \n",
    "    # 指标所需的方向：“min”或“max”\n",
    "    direction=\"max\") \n",
    "    tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=objective,\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-turtle",
   "metadata": {},
   "source": [
    "最后，让我们开始搜索。 不要忘记传递验证数据，并确保不要将您的测试集用作验证数据——否则您很快就会开始过度拟合您的测试数据，并且您将无法再信任您的测试指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 62 Complete [00h 01m 21s]\n",
      "val_accuracy: 0.9741999804973602\n",
      "\n",
      "Best val_accuracy So Far: 0.9759999811649323\n",
      "Total elapsed time: 01h 19m 09s\n",
      "\n",
      "Search: Running Trial #63\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units             |64                |64                \n",
      "optimizer         |rmsprop           |rmsprop           \n",
      "\n",
      "Epoch 1/100\n",
      "391/391 - 2s - loss: 0.4198 - accuracy: 0.8880 - val_loss: 0.2273 - val_accuracy: 0.9387\n",
      "Epoch 2/100\n",
      "391/391 - 2s - loss: 0.2124 - accuracy: 0.9400 - val_loss: 0.1861 - val_accuracy: 0.9487\n",
      "Epoch 3/100\n",
      "391/391 - 2s - loss: 0.1661 - accuracy: 0.9524 - val_loss: 0.1467 - val_accuracy: 0.9593\n",
      "Epoch 4/100\n",
      "391/391 - 2s - loss: 0.1370 - accuracy: 0.9616 - val_loss: 0.1375 - val_accuracy: 0.9614\n",
      "Epoch 5/100\n",
      "391/391 - 2s - loss: 0.1174 - accuracy: 0.9660 - val_loss: 0.1226 - val_accuracy: 0.9663\n",
      "Epoch 6/100\n",
      "391/391 - 2s - loss: 0.1015 - accuracy: 0.9706 - val_loss: 0.1118 - val_accuracy: 0.9681\n",
      "Epoch 7/100\n",
      "391/391 - 2s - loss: 0.0896 - accuracy: 0.9742 - val_loss: 0.1042 - val_accuracy: 0.9682\n",
      "Epoch 8/100\n",
      "391/391 - 2s - loss: 0.0797 - accuracy: 0.9774 - val_loss: 0.0970 - val_accuracy: 0.9734\n",
      "Epoch 9/100\n",
      "391/391 - 2s - loss: 0.0714 - accuracy: 0.9793 - val_loss: 0.0980 - val_accuracy: 0.9721\n",
      "Epoch 10/100\n",
      "391/391 - 2s - loss: 0.0650 - accuracy: 0.9811 - val_loss: 0.0959 - val_accuracy: 0.9730\n",
      "Epoch 11/100\n",
      "391/391 - 2s - loss: 0.0591 - accuracy: 0.9823 - val_loss: 0.0941 - val_accuracy: 0.9731\n",
      "Epoch 12/100\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "\n",
    "# 保留这些以备后用\n",
    "x_train_full = x_train[:] \n",
    "y_train_full = y_train[:] \n",
    "\n",
    "\n",
    "# 留出一个验证集\n",
    "num_val_samples = 10000 \n",
    "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:] \n",
    "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:] \n",
    "\n",
    "# 使用大量 epoch（您事先不知道。每个模型需要多少个 epoch）\n",
    "# 并在开始过度拟合时使用 EarlyStopping 回调来停止训练。\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5), ]\n",
    "\n",
    "# 这采用与 fit() 相同的参数（它简单地将它们传递给每个新模型的 fit() ）。\n",
    "tuner.search( \n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100, \n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-stadium",
   "metadata": {},
   "source": [
    "上面的例子将在几分钟内运行，因为我们只考虑了几个可能的选择，而且我们正在 MNIST 上进行训练。 但是，对于典型的搜索空间和典型的数据集，您经常会发现自己让超参数搜索在一夜之间甚至几天内运行。 如果您的搜索过程崩溃，您可以随时重新启动它——只需在调谐器中指定 overwrite=False 以便它可以从存储在磁盘上的试验日志中恢复。\n",
    "\n",
    "搜索完成后，您可以查询最佳超参数配置，您可以使用这些配置创建高性能模型，然后重新训练："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-subject",
   "metadata": {},
   "source": [
    "> 清单 13.3 查询最佳超参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 4\n",
    "# 返回 HyperParameters 对象的列表，您可以将其传递给建模函数。\n",
    "best_hps = tuner.get_best_hyperparameters(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-cameroon",
   "metadata": {},
   "source": [
    "通常，在重新训练这些模型时，您可能希望将验证数据作为训练数据的一部分，因为您不会进行进一步的超参数更改，因此您将不再评估验证数据的性能。 在我们的示例中，我们将在原始 MNIST 训练数据的整体上训练这些最终模型，而不保留验证集。\n",
    "\n",
    "但是，在我们可以对完整的训练数据进行训练之前，我们需要确定最后一个参数：要训练的最佳 epoch 数。 通常，您希望训练新模型的时间比搜索期间更长：在 EarlyStopping 回调中使用积极的耐心值可以节省搜索期间的时间，但可能会导致模型欠拟合。 只需使用验证集即可找到最佳周期："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_epoch(hp):\n",
    "    model = build_model(hp)\n",
    "#     请注意非常高的耐心值。\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)]\n",
    "    \n",
    "    history = model.fit(x_train, \n",
    "                        y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=100,\n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    val_loss_per_epoch = history.history[\"val_loss\"]\n",
    "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-miami",
   "metadata": {},
   "source": [
    "最后，在整个数据集上训练比这个 epoch 计数长一点的时间，因为你正在训练更多的数据——在这种情况下多 20%："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trained_model(hp):\n",
    "    best_epoch = get_best_epoch(hp)\n",
    "    model.fit(x_train_full, \n",
    "              y_train_full,\n",
    "              batch_size=128, \n",
    "              epochs=int(best_epoch * 1.2))\n",
    "    return model\n",
    "\n",
    "best_models = []\n",
    "for hp in best_hps:\n",
    "    model = get_best_trained_model(hp)\n",
    "    model.evaluate(x_test, y_test)\n",
    "    best_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-component",
   "metadata": {},
   "source": [
    "请注意，如果您不担心性能稍差，您可以采取一条捷径：只需使用调谐器重新加载性能最佳的模型，并在超参数搜索期间保存最佳权重，而无需从头开始重新训练新模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner.get_best_models(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-estate",
   "metadata": {},
   "source": [
    "> 打造正确搜索空间的艺术在大规模进行自动超参数优化时要记住的一个重要问题是验证集过度拟合。 因为您正在根据使用验证数据计算的信号更新超参数，所以您可以在验证数据上有效地训练它们，因此它们会很快过拟合验证数据。 始终牢记这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-dinner",
   "metadata": {},
   "source": [
    "**打造正确搜索空间的艺术**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-holocaust",
   "metadata": {},
   "source": [
    "总的来说，超参数优化是一项强大的技术，是在任何任务上获得最先进的模型或赢得机器学习竞赛的绝对必要条件。想一想：曾几何时，人们手工制作了进入浅层机器学习模型的特征。这是非常次优的。现在，深度学习使分层特征工程的任务自动化——特征是使用反馈信号学习的，而不是手动调整的，这就是它应该的方式。同样，您不应该手工制作模型架构；您应该以有原则的方式优化它们。\n",
    "\n",
    "然而，进行超参数调整并不能替代熟悉模型架构最佳实践：搜索空间随着选择的数量而组合增长，因此将所有内容都转换为超参数并让调整器对其进行分类的成本太高了。您需要聪明地设计正确的搜索空间。超参数调优是自动化，而不是魔术：您可以使用它来自动化原本需要手动运行的实验，但您仍然需要精心挑选有可能产生良好指标的实验配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-somewhere",
   "metadata": {},
   "source": [
    "好消息：通过利用超参数调整，您必须做出的配置决策从微观决策（我为这一层选择多少单元？）升级到更高级别的架构决策（我应该在整个模型中使用残差连接吗？ ）。 虽然微决策特定于某个模型和某个数据集，但更高级别的决策可以更好地跨不同任务和数据集泛化：例如，几乎每个图像分类问题都可以通过相同类型的搜索空间模板解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-poland",
   "metadata": {},
   "source": [
    "遵循这一逻辑，KerasTuner 尝试提供与广泛类别的问题（例如图像分类）相关的预制搜索空间。 只需添加数据，运行搜索，即可获得一个非常好的模型。 您可以尝试超模型 kt.applications.HyperXception 和 kt.applications.HyperResNet，它们是 Keras 应用程序模型的有效可调版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-thread",
   "metadata": {},
   "source": [
    "**超参数调整的未来：自动化机器学习**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-poker",
   "metadata": {},
   "source": [
    "目前，作为深度学习工程师，您的大部分工作包括使用 Python 脚本处理数据，然后详细调整深度网络的架构和超参数以获得工作模型，甚至获得最先进的模型 模型，如果你有那么大的野心。 不用说，这不是最佳设置。 但是自动化可以提供帮助，它不会仅仅停留在超参数调整上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-authentication",
   "metadata": {},
   "source": [
    "搜索一组可能的学习率或可能的层大小只是第一步。 我们也可以更加雄心勃勃，尝试从头开始生成模型架构本身，限制尽可能少：例如，通过强化学习或遗传算法。 未来，整个端到端 ML 管道将自动生成，而不是由工程师手工制作。 这称为自动机器学习或 AutoML\n",
    ". 您已经可以利用 AutoKeras 之类的库：解决基本的 ML 问题，而您几乎不需要参与。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-opera",
   "metadata": {},
   "source": [
    "今天，AutoML 仍处于早期阶段，不能扩展到大问题。 但是，当 AutoML 成熟到可以被广泛采用时，机器学习工程师的工作不会消失——而是工程师将向价值创造链的上游移动。 他们将开始在数据管理上投入更多精力，制定真正反映业务目标的复杂损失函数，并了解他们的模型如何影响部署他们的数字生态系统（例如，使用模型预测的用户） 并生成模型的训练数据）——目前只有最大的公司才能考虑的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-farming",
   "metadata": {},
   "source": [
    "始终着眼大局，专注于理解基本面，并记住高度专业化的单调乏味最终将被自动化。 将其视为礼物——为您的工作流程提高生产力——而不是对您自己的相关性构成威胁。 无休止地调整旋钮不应该是你的工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-accused",
   "metadata": {},
   "source": [
    "### 模型集成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-member",
   "metadata": {},
   "source": [
    "另一种获得最佳任务结果的强大技术是模型集成。 集成包括将一组不同模型的预测汇集在一起以产生更好的预测。 如果您查看机器学习竞赛，特别是 Kaggle 上的竞赛，您会发现获胜者使用了非常大的模型集合，这些模型不可避免地击败了任何单个模型，无论模型有多好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-carpet",
   "metadata": {},
   "source": [
    "集成依赖于这样一个假设，即独立训练的不同表现良好的模型可能出于不同的原因是好的：每个模型查看数据的稍微不同的方面来进行预测，获得部分“真相”但不是全部。你可能熟悉盲人与大象的古老寓言：一群盲人第一次遇到大象，并试图通过触摸它来了解大象是什么。每个人接触大象身体的不同部位——只是一个部位，比如躯干或腿。然后男人们互相描述大象是什么：“它像一条蛇”、“像一根柱子或一棵树”等等。盲人本质上是机器学习模型，试图理解训练数据的多样性，每个模型都从自己的角度出发，使用自己的假设（由模型的独特架构和独特的随机权重初始化提供）。他们每个人都获得了数据的一部分真相，但不是全部真相。通过汇集他们的观点，您可以获得对数据的更准确的描述。大象是多个部分的组合：没有一个盲人完全正确，但是，一起接受采访时，他们可以讲述一个相当准确的故事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-feedback",
   "metadata": {},
   "source": [
    "我们以分类为例。 汇集一组分类器的预测（以集成分类器）的最简单方法是在推理时平均它们的预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用四种不同的模型来计算初始预测。\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "\n",
    "# 这个新的预测阵列应该比任何初始预测阵列更准确。\n",
    "final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-negative",
   "metadata": {},
   "source": [
    "然而，这只有在分类器或多或少同样好时才有效。 如果其中一个明显比另一个差，则最终的预测可能不如该组中最好的分类器。 集成分类器的一个更聪明的方法是进行加权平均，其中权重是在验证数据上学习的——通常，更好的分类器被赋予更高的权重，而较差的分类器被赋予更低的权重。 要搜索一组好的集成权重，您可以使用随机搜索或简单的优化算法，例如 Nelder-Mead 算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "# 假设这些权重 (0.5, 0.25, 0.1, 0.15) 是根据经验学习的。\n",
    "final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-polls",
   "metadata": {},
   "source": [
    "有许多可能的变体：例如，您可以对预测的指数进行平均。 通常，对验证数据优化权重的简单加权平均值提供了非常强大的基线。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-enzyme",
   "metadata": {},
   "source": [
    "使集成工作的关键是分类器的集合。多样性就是力量。多样性 如果所有的盲人都只摸过大象的鼻子，他们会同意大象就像蛇，他们将永远不知道大象的真相。多样性是使集成工作的原因。在机器学习方面，如果您的所有模型都以相同的方式存在偏差，那么您的集成将保留相同的偏差。如果您的模型以不同的方式存在偏差，偏差将相互抵消，并且集成将更加稳健和准确。\n",
    "\n",
    "出于这个原因，您应该集成尽可能好同时尽可能不同的模型。这通常意味着使用非常不同的架构，甚至不同品牌的机器学习方法。在很大程度上不值得做的一件事是，从不同的随机初始化中将经过多次独立训练的同一个网络集成在一起。如果您的模型之间的唯一区别是它们的随机初始化和它们暴露于训练数据的顺序，那么您的集成将是低多样性的，并且仅比任何单个模型提供微小的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-innocent",
   "metadata": {},
   "source": [
    "我发现在实践中行之有效的一件事是使用基于树的方法（例如随机森林或梯度提升树）和深度神经网络的集合，但这并不适用于所有问题域。 2014 年，Andrei Kolev 和我在 Kaggle (www.kaggle.com/c/higgs-boson) 的希格斯玻色子衰变检测挑战中使用各种树模型和深度神经网络的集合获得了第四名。值得注意的是，集成中的一个模型源自与其他模型不同的方法（它是正则化的贪婪森林），并且得分明显低于其他模型。不出所料，它在整体中被赋予了很小的权重。但令我们惊讶的是，结果证明它大大提高了整体的集成度，因为它与其他模型大不相同：它提供了其他模型无法访问的信息。这正是集成的重点。重要的不是你最好的模型有多好；这是关于你的候选模型集的多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-underground",
   "metadata": {},
   "source": [
    "## 扩大模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-tours",
   "metadata": {},
   "source": [
    "回想我们在第 7 章中介绍的“进度循环”概念：您的想法的质量取决于它们经历了多少改进周期。 您对一个想法进行迭代的速度取决于您设置实验的速度、运行该实验的速度以及最终分析结果数据的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-clothing",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gt8r350oksj31800sy414.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-denver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
