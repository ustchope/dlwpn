{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "under-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.3 ms (started: 2021-08-08 21:29:56 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\tgit@github.com:ustchope/dlwpn.git (fetch)\n",
      "origin\tgit@github.com:ustchope/dlwpn.git (push)\n",
      "[master 665c2de] 更新 ch14 #1 change Aug 08, 2021\n",
      " 2 files changed, 228 insertions(+), 141 deletions(-)\n",
      " create mode 100644 ch14.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:ustchope/dlwpn.git\n",
      "   30fd126..665c2de  master -> master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.02 s (started: 2021-08-08 21:30:31 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 ch14 #1 change Aug 08, 2021'\n",
    "\n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[1] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-identifier",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-hundred",
   "metadata": {},
   "source": [
    "**本章包含**\n",
    "\n",
    "* 本书的重要内容\n",
    "* 深度学习的局限性\n",
    "* 深度学习、机器学习和人工智能未来可能的方向\n",
    "* 用于进一步学习和在实践中应用技能的资源"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-commonwealth",
   "metadata": {},
   "source": [
    "你几乎读到了这本书的结尾。最后一章将总结和回顾核心概念，同时还将您的视野扩展到迄今为止所学的范围之外。成为一名有效的人工智能从业者是一个旅程，完成本书只是你迈出的第一步。我想确保你意识到这一点，并有足够的装备来独自完成这个旅程的下一步。\n",
    "\n",
    "我们将从您应该从本书中学到的东西的鸟瞰图开始。这应该会刷新你对你学到的一些概念的记忆。接下来，我们将概述深度学习的一些关键限制。要适当地使用工具，您不仅应该了解它的作用，还应该了解它的作用。最后，我将提供一些关于深度学习、机器学习和 AI 的未来演变的无法推测的想法。如果你想进入基础研究，这对你来说应该特别有趣。本章的结尾是一个简短的资源和策略列表，用于进一步学习机器学习和跟上最新进展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-connecticut",
   "metadata": {},
   "source": [
    "## 关键概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-toddler",
   "metadata": {},
   "source": [
    "本节简要综合了本书的主要内容。 如果您需要快速复习以帮助回忆所学内容，可以阅读以下几页。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-least",
   "metadata": {},
   "source": [
    "### 人工智能的各种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-congo",
   "metadata": {},
   "source": [
    "首先，深度学习不是人工智能的同义词，甚至不是机器学习的同义词。\n",
    "* 人工智能 (AI) 是一个古老而广泛的领域，通常可以理解为“所有使人类认知过程自动化的尝试”。这可以从非常基本的（例如 Excel 电子表格）到非常高级的（例如可以走路和说话的人形机器人）。\n",
    "* 机器学习是 AI 的一个特定子领域，旨在完全从训练数据中自动开发程序（称为 ）。这种将模型数据转换为程序的过程称为 。尽管机器学习已经存在了很长时间，但它在 1990 年代才开始起飞，然后在 2000 年代成为 AI 的主要形式。\n",
    "* 深度学习是机器学习的众多分支之一，其中模型是几何变换的长链，一个接一个地应用。这些操作被组织成模块，称为：深度学习模型通常是层的堆栈——或者更一般地说，层的图。这些层通过权重进行参数化，权重是训练期间学习的参数。模型的权重存储在知识中，学习过程包括为这些权重找到“好的值”——最小化损失函数的值。由于所考虑的几何变换链是可微的，因此通过梯度下降可以有效地更新权重以最小化损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-corporation",
   "metadata": {},
   "source": [
    "尽管深度学习只是机器学习的众多方法之一，但它与其他方法并不平等。 深度学习取得了突破性的成功。 这是为什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-stake",
   "metadata": {},
   "source": [
    "### 是什么让深度学习在机器学习领域与众不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-beast",
   "metadata": {},
   "source": [
    "在短短几年的时间里，深度学习在广泛的任务上取得了巨大的突破，这些任务在历史上被认为对计算机来说是极其困难的，尤其是在机器感知领域：从图像、视频、声音、 和更多。 给定足够的训练数据（特别是由人类适当标记的训练数据），深度学习使得从感知数据中提取几乎任何人类能够提取的数据成为可能。 因此，有时会说深度学习“解决了感知”——尽管这仅适用于相当狭隘的感知定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-breakdown",
   "metadata": {},
   "source": [
    "由于其前所未有的技术成功，深度学习单枪匹马地带来了第三个也是迄今为止最大的：在人工智能的人工智能夏季领域中的一个被强烈关注、投资和炒作的时期。本书正在撰写中，我们正处于其中。这个时期是否会在不久的将来结束，结束后会发生什么，都是争论的话题。有一件事是肯定的：与之前的人工智能夏天形成鲜明对比的是，深度学习为大大小小的科技公司提供了巨大的商业价值，实现了人类级别的语音识别、智能助手、人类级别的图像分类、极大改进的机器翻译，以及更多的。炒作可能（并且可能会）消退，但深度学习的持续经济和技术影响将继续存在。从这个意义上说，深度学习类似于互联网：它可能在几年内被过度炒作，但从长远来看，它仍将是一场改变我们经济和生活的重大革命。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-longer",
   "metadata": {},
   "source": [
    "我对深度学习特别乐观，因为即使我们在接下来的十年里不会取得进一步的技术进步，将现有算法部署到每个适用的问题上，对大多数行业来说都会改变游戏规则。 深度学习无外乎是一场革命，由于对资源和员工的指数投资，目前正在以令人难以置信的速度取得进展。 从我的立场来看，未来看起来是光明的，尽管短期预期有些过于乐观； 充分发挥深度学习的潜力可能需要几十年的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-consciousness",
   "metadata": {},
   "source": [
    "### 如何思考深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-invasion",
   "metadata": {},
   "source": [
    "深度学习最令人惊讶的是它是多么的简单。 十年前，没有人预料到我们会通过使用经过梯度下降训练的简单参数模型在机器感知问题上取得如此惊人的结果。 现在，事实证明，您所需要的只是在足够多的示例上使用梯度下降训练的足够大的参数模型。 正如费曼曾经对宇宙所说的那样，“它并不复杂，它只是很多。”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-barbados",
   "metadata": {},
   "source": [
    "在深度学习中，一切都是一个向量——也就是说，一切都是一个点几何空间中的一个。模型输入（文本、图像等）和目标首先：转换为初始向量化输入向量空间和目标向量空间。深度学习模型中的每一层都对通过它的数据进行一个简单的几何变换。模型中的层链一起形成一个复杂的几何变换，分解为一系列简单的几何变换。这种复杂的转换试图一次一个点地将输入空间映射到目标空间。这种转换由层的权重参数化，这些层的权重根据模型当前的表现进行迭代更新。这种几何变换的一个关键特征是它必须是可微的，这是我们能够通过梯度下降学习其参数所必需的。直观地说，这意味着从输入到输出的几何变形必须平滑且连续——这是一个重要的约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-arrow",
   "metadata": {},
   "source": [
    "将这种复杂的几何变换应用于输入数据的整个过程可以通过想象一个人试图解开一个纸球来在 3D 中可视化：被弄皱的纸球是模型开始的输入数据的流形。 人在纸球上操作的每一个动作都类似于一层操作的简单几何变换。 完整的解压手势序列是整个模型的复杂变换。 深度学习模型是用于解开复杂的高维数据流形的数学机器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-performance",
   "metadata": {},
   "source": [
    "这就是深度学习的神奇之处：将意义转化为向量，转化为几何空间，然后逐步学习将一个空间映射到另一个空间的复杂几何变换。 您所需要的只是具有足够高维数的空间，以便捕获原始数据中发现的所有关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-lodge",
   "metadata": {},
   "source": [
    "整个事情取决于一个核心思想：意义源自事物之间的成对关系（语言中的单词之间、图像中的像素之间等），并且这些关系可以通过距离函数来捕获。但请注意，大脑是否通过几何空间实现意义是一个完全不同的问题。从计算的角度来看，向量空间的工作效率很高，但很容易想象不同的智能数据结构——尤其是图。神经网络最初源于使用图作为编码意义的方式的想法，这就是它们被命名为神经网络的原因；周围的研究领域曾经被称为联结主义。如今，“神经网络”这个名称的存在纯粹是出于历史原因——这是一个极具误导性的名称，因为它们既不是神经网络，也不是网络。特别是，神经网络与大脑几乎没有任何关系。更合适的名称应该是分层表示学习或分层表示学习，或者甚至是链式或几何变换的深度可微模型，以强调连续几何空间操作是其核心的事实。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-system",
   "metadata": {},
   "source": [
    "### 关键使能技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-buffer",
   "metadata": {},
   "source": [
    "目前正在展开的技术革命并非始于任何一项突破性的发明。 相反，与任何其他革命一样，它是大量促成因素的产物——起初是缓慢的，然后是突然的。 在深度学习的情况下，我们可以指出以下关键因素：\n",
    "* 增量算法创新，首先传播了二十多年（从反向传播开始），然后随着 2012 年之后更多的研究工作投入到深度学习中，发生的速度越来越快。\n",
    "* 大量感知数据的可用性，这是实现在足够大的数据上训练的足够大的模型的必要条件。这反过来又是消费互联网兴起和应用于存储媒体的摩尔定律的副产品。\n",
    "* 以低廉的价格提供快速、高度并行的计算硬件，尤其是 NVIDIA 生产的 GPU——首先是游戏 GPU，然后是从头开始为深度学习设计的芯片。早些时候，NVIDIA 首席执行官 Jensen Huang 注意到深度学习的热潮，并决定将公司的未来押注于此——这在大路。\n",
    "* 一个复杂的软件层堆栈，使人类可以使用这种计算能力：CUDA 语言，像 TensorFlow 这样的自动微分框架，以及使大多数人可以使用深度学习的 Keras。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-expansion",
   "metadata": {},
   "source": [
    "未来，深度学习将不仅仅被专家——研究人员、研究生和具有学术背景的工程师使用——它将成为每个开发人员工具箱中的工具，就像今天的网络技术一样。 每个人都需要构建智能应用程序：就像今天的每个企业都需要一个网站一样，每个产品都需要智能地理解用户生成的数据。 实现这个未来将需要我们构建工具，使任何具有基本编码能力的人都可以轻松使用和访问深度学习。 Keras 是朝这个方向迈出的第一个重要步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-minimum",
   "metadata": {},
   "source": [
    "### 通用机器学习工作流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-projection",
   "metadata": {},
   "source": [
    "可以使用极其强大的工具来创建将任何输入空间映射到任何目标空间的模型，这很棒，但机器学习工作流程的困难部分通常是设计和训练此类模型之前的所有内容（对于生产模型， 之后是什么）。 了解问题域以便能够确定尝试预测什么、给定什么数据以及如何衡量成功，这是任何机器学习成功应用的先决条件，而不是像 Keras 和 TensorFlow 可以帮助您。 提醒一下，这里是第 6 章中描述的典型机器学习工作流程的快速摘要：\n",
    "1. 定义问题：哪些数据是可用的，你想预测什么？您是否需要收集更多数据，或雇用人员手动标记数据集？\n",
    "2. 确定一种可靠衡量目标成功的方法。对于简单的任务，这可能是预测准确性，但在许多情况下，它需要复杂的、特定于领域的指标。\n",
    "3. 准备用于评估模型的验证过程。特别是，您应该定义一个训练集、一个验证集和一个测试集。验证集和测试集标签不应泄漏到训练数据中：例如，对于时间预测，验证和测试数据应该在训练数据之后。\n",
    "4. 通过将数据转换为向量并对其进行预处理以使其更容易被神经网络（归一化等）处理，从而对数据进行矢量化处理。\n",
    "5. 开发第一个超越普通常识基线的模型，从而证明机器学习可以解决您的问题。情况可能并非总是如此！\n",
    "6. 通过调整超参数和添加正则化来逐步完善您的模型架构。仅根据验证数据的性能进行更改，而不是测试数据或训练数据。请记住，您应该让您的模型过度拟合（从而确定一个大于您需要的模型容量级别），然后才开始添加正则化或缩小您的模型。在转换超参数时要注意验证集过度拟合：事实上，您的超参数最终可能会过度专业化到验证集。避免这种情况是拥有单独测试集的目的！\n",
    "7. 在生产环境中部署您的最终模型——作为 Web API、作为 JavaScript 或 C++ 应用程序的一部分、在嵌入式设备上等。继续监控其在实际数据上的性能，并使用您的发现来改进模型的下一次迭代 ！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-wednesday",
   "metadata": {},
   "source": [
    "## 关键网络架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-society",
   "metadata": {},
   "source": [
    "你应该熟悉的四类网络架构是密集连接网络、卷积网络、循环网络和Transformers。 每种类型的模型都适用于特定的输入模式：网络架构对数据结构的假设进行编码：一个假设空间，在其中搜索良好模型的过程将继续进行。 给定的架构能否解决给定的问题完全取决于数据结构与网络架构假设之间的匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-preliminary",
   "metadata": {},
   "source": [
    "这些不同的网络类型可以轻松组合以实现更大的多模式模型，就像组合乐高积木一样。 在某种程度上，深度学习层是用于信息处理的乐高积木。 以下是输入模式和适当网络架构之间映射的快速概览：\n",
    "* 矢量数据——密集连接的模型（密集层）。\n",
    "* 图像数据——2D convnets。\n",
    "* 序列数据——用于时间序列的 RNN，或用于离散序列（例如单词序列）的 Transformer。 1D convnets 也可用于平移不变的连续序列数据，例如鸟鸣波形。\n",
    "* 视频数据 - 3D convnets（如果您需要捕捉运动效果）或用于特征提取的帧级2D convnets的组合，然后是序列处理模型。\n",
    "* 体积数据——3D convnets。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-check",
   "metadata": {},
   "source": [
    "现在，让我们快速回顾一下每种网络架构的特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-world",
   "metadata": {},
   "source": [
    "**DENSELY-CONNECTED NETWORKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-roberts",
   "metadata": {},
   "source": [
    "密集连接的网络是一堆密集层，用于处理向量数据（其中每个样本都是数字或分类属性的向量）。 这样的网络在输入特征中没有假设特定的结构：它们被称为密集连接，因为密集层的单元与其他每个单元相连。 该层尝试映射任意两个输入要素之间的关系； 例如，这与仅查看局部关系的 2D 卷积层不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-episode",
   "metadata": {},
   "source": [
    "密集连接网络最常用于分类数据（例如，其中输入特征是属性列表），例如第 4 章中使用的波士顿住房价格数据集。它们也用作 大多数网络。 例如，第 8 章中介绍的卷积神经网络通常以一层或两层结尾，第 10 章中的密集循环网络也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-system",
   "metadata": {},
   "source": [
    "请记住：要执行二元分类，请使用具有单个单元和 sigmoid 激活的 Dense 层结束您的层堆栈，并使用 binary_crossentropy 作为损失。 你的目标应该是 0 或 1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(num_input_features,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-airline",
   "metadata": {},
   "source": [
    "要执行单标签分类分类（其中每个样本只有一个类，没有更多），请以一个单元数等于 Dense 类数和 softmax 激活的层结束您的层堆栈。 如果您的目标是单热编码，请使用 categorical_crossentropy 作为损失； 如果它们是整数，请使用 sparse_categorical_ crossentropy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_input_features,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-singer",
   "metadata": {},
   "source": [
    "要执行多标签分类分类（其中每个样本可以有多个类），请使用 Dense 层结束您的层堆栈，该层的单位数等于类数和 sigmoid 激活，并使用 binary_crossentropy 作为损失。 你的目标应该是 k-hot 编码的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_input_features,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-multiple",
   "metadata": {},
   "source": [
    "要对连续值向量执行回归，请使用 Dense 层结束您的层堆栈，该层的单位数等于您尝试预测的值的数量（通常是单个值，例如房屋的价格） ，并且没有激活。 各种损失可用于回归——最常见的是 mean_squared_error (MSE)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_input_features,))\n",
    "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs layers.Dense(num_values)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-transaction",
   "metadata": {},
   "source": [
    "**CONVNETS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-appearance",
   "metadata": {},
   "source": [
    "卷积层通过对输入张量中的不同空间位置（补丁）应用相同的几何变换来查看空间局部模式。 这导致表示具有平移不变性，使卷积层具有高度的数据效率和模块化。 这个想法适用于任何维度的空间：1D（连续序列）、2D（图像）、3D（体积）等等。 您可以使用 Conv1D 层来处理序列，使用 Conv2D 层来处理图像，以及使用 Conv3D 层来处理体积。 作为卷积层的更精简、更有效的替代方案，您还可以使用深度可分离卷积层，例如 SeparableConv2D。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-accommodation",
   "metadata": {},
   "source": [
    "Convnets 或卷积网络由卷积层和最大池化层组成。 池化层允许您对数据进行空间下采样，随着特征数量的增加，这需要将特征图保持在合理的大小，并允许后续卷积层“看到”输入的更大空间范围。 Convnet 通常以 Flatten 操作或全局池化层结束，将空间特征图转换为向量，然后是 Dense 层以实现分类或回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-toddler",
   "metadata": {},
   "source": [
    "这是一个典型的图像分类网络（在这种情况下是分类分类），利用了 SeparableConv2D 层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(height, width, channels))\n",
    "x = layers.SeparableConv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
    "x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
    "x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-offering",
   "metadata": {},
   "source": [
    "在构建非常深的 convnet 时，通常会添加批量归一化层以及残差连接——这两种架构模式有助于梯度信息在网络中顺畅流动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-forth",
   "metadata": {},
   "source": [
    "**RNNS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-scheme",
   "metadata": {},
   "source": [
    "循环神经网络 (RNN) 通过一次一个时间步处理输入序列并始终保持状态（状态通常是一个向量或一组向量）来工作。 在感兴趣的模式不随时间转换而变化的序列（例如，最近的过去比遥远的过去更重要的时间序列数据）的序列的情况下，它们应该优先于 1D convnets 使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-supplement",
   "metadata": {},
   "source": [
    "Keras 中提供了三个 RNN 层：SimpleRNN、GRU 和 LSTM。 对于大多数实际用途，您应该使用 GRU 或 LSTM。 LSTM 是两者中功能更强大的一个，但也更昂贵； 您可以将 GRU 视为更简单、更便宜的替代品。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-making",
   "metadata": {},
   "source": [
    "为了将多个 RNN 层相互叠加，堆栈中最后一层之前的每一层都应返回其输出的完整序列（每个输入时间步长将对应一个输出时间步长）； 如果您没有堆叠任何其他 RNN 层，那么通常只返回最后一个输出，其中包含有关整个序列的信息。\n",
    "\n",
    "以下是用于向量序列二元分类的单个 RNN 层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_timesteps, num_features))\n",
    "x = layers.LSTM(32)(inputs)\n",
    "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-baking",
   "metadata": {},
   "source": [
    "这是一个用于向量序列二元分类的堆叠 RNN 层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(num_timesteps, num_features))\n",
    "x = layers.LSTM(32, return_sequences=True)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "x = layers.LSTM(32)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-evolution",
   "metadata": {},
   "source": [
    "**TRANSFORMERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-glossary",
   "metadata": {},
   "source": [
    "Transformer 查看一组向量（例如词向量），并利用神经注意力将每个向量转换为一种表示，该表示知道该组中其他上下文向量所提供的。 当所讨论的集合是有序序列时，您还可以利用位置编码来创建可以同时考虑全局上下文和词序的 Transformer，能够比 RNN 或 1D convnet 更有效地处理长文本段落。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-treaty",
   "metadata": {},
   "source": [
    "Transformer 可用于任何集合处理或序列处理任务，包括文本分类，但它们尤其擅长序列到序列学习，例如将源语言中的段落翻译成目标语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-equality",
   "metadata": {},
   "source": [
    "一个序列到序列的 Transformer 由两部分组成：\n",
    "* TransformerEncoder 将输入向量序列转换为上下文感知、顺序感知的输出向量序列。\n",
    "* 一个 TransformerDecoder，它接受 TransformerDecoder 的输出以及目标序列，并预测目标序列中接下来应该发生什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-cuisine",
   "metadata": {},
   "source": [
    "如果您只处理单个序列（或一组）向量，则只能使用 TransformerEncoder。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-introduction",
   "metadata": {},
   "source": [
    "以下是用于将源序列映射到目标序列的序列到序列转换器（例如，此设置可用于机器翻译或问答）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源序列\n",
    "encoder_inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\") \n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# 到目前为止的目标序列\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\") \n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "\n",
    "# 目标序列未来一步\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "transformer.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-laser",
   "metadata": {},
   "source": [
    "这是一个单独的 TransformerEncoder，用于整数序列的二进制分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-bonus",
   "metadata": {},
   "source": [
    "第 11 章提供了 TransformerEncoder、TransformerDecoder 以及 PositionalEmbedding 层的完整实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-spice",
   "metadata": {},
   "source": [
    "### 可能性空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-rebel",
   "metadata": {},
   "source": [
    "您将使用这些技术构建什么？ 请记住，构建深度学习模型就像玩乐高积木：只要您有适当的可用训练数据，并且可以通过合理复杂性的连续几何变换来实现映射，层可以连接在一起以将基本上任何东西映射到任何东西。 可能性的空间是无限的。 本节提供了一些示例，以激发您超越传统上作为机器学习基础的基本分类和回归任务进行思考。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-gates",
   "metadata": {},
   "source": [
    "我已经按输入和输出方式对我建议的应用程序进行了排序。 请注意，其中有很多扩展了可能的限制——尽管模型可以在所有这些任务上进行训练，但在某些情况下，这样的模型可能无法从其训练数据中广泛推广。 第 14.2、14.3 和 14.4 节将讨论未来如何解除这些限制。\n",
    "* 将矢量数据映射到矢量数据\n",
    "    - 预测性医疗保健——将患者医疗记录映射到对患者结果的预测\n",
    "    - 行为定位——用用户在网站上花费多长时间的数据映射一组网站属性\n",
    "    - 产品质量控制——映射一组与制造产品实例相关的属性，以及该产品在明年之前失败的概率\n",
    "* 将图像数据映射到矢量数据\n",
    "    - 医疗助理 - 映射医学图像的幻灯片，预测肿瘤的存在\n",
    "    - 自动驾驶汽车——将汽车行车记录仪视频帧映射到方向盘角度命令以及油门和刹车命令\n",
    "    - 棋盘游戏 AI——将围棋或国际象棋棋盘映射到下一个玩家移动\n",
    "    - 饮食助手 - 将菜肴的图片映射到其卡路里计数\n",
    "    - 年龄预测——将自拍映射到人的年龄\n",
    "* 将时间序列数据映射到向量数据\n",
    "    - 天气预测 - 在特定位置的下一周天气数据位置网格中映射天气数据的时间序列\n",
    "    - 脑机接口——将脑磁图 (MEG) 数据的时间序列映射到计算机命令\n",
    "    - 行为定位——将网站上用户交互的时间序列映射到用户购买某物的概率\n",
    "* 将文本映射到文本\n",
    "    * 机器翻译——将一种语言的段落映射到另一种语言的翻译版本\n",
    "    * 智能回复——将电子邮件映射到可能的单行回复\n",
    "    * 问答——将一般知识问题映射到答案\n",
    "    * 总结——将一篇长文章映射到文章的简短摘要\n",
    "* 将图像映射到文本\n",
    "    * 文本转录 - 将包含文本元素的图像映射到相应的文本字符串\n",
    "    * 字幕——将图像映射到描述图像内容的短标题\n",
    "* 将文本映射到图像\n",
    "    * 条件图像生成——将简短的文本描述映射到与描述匹配的图像\n",
    "    * 标志生成/选择——将公司的名称和描述映射到标志建议\n",
    "* 将图像映射到图像\n",
    "    * 超分辨率——将缩小的图像映射到相同图像的更高分辨率版本\n",
    "    * 视觉深度感知——将室内环境的图像映射到深度预测的地图\n",
    "* 将图像和文本映射到文本\n",
    "    * 视觉质量保证——映射图像和关于内容的自然语言问题\n",
    "    * 图像到自然语言答案\n",
    "* 将视频和文本映射到文本\n",
    "    * 视频 QA——将关于视频内容的短视频和自然语言问题映射到自然语言答案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-maryland",
   "metadata": {},
   "source": [
    "几乎一切皆有可能——但并非完全如此。 让我们在接下来的任何部分中看看我们不能用深度学习做什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-message",
   "metadata": {},
   "source": [
    "## 深度学习的局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-ferry",
   "metadata": {},
   "source": [
    "深度学习可以实现的应用空间是无限的。然而，对于当前的深度学习技术来说，许多应用程序仍然完全无法实现——即使有大量人工标注的数据。例如，假设您可以组装一个数据集，其中包含由产品经理编写的数十万甚至数百万个软件产品特性的英语描述，以及由一个团队开发的相应源代码。工程师来满足这些要求。即使使用这些数据，您也无法训练深度学习模型来读取产品描述并生成适当的代码库。这只是众多例子中的一个。一般来说，任何需要推理的东西——比如编程或应用科学方法——长期规划和算法数据操作对于深度学习模型来说都是遥不可及的，无论你向它们扔多少数据。即使是用深度神经网络学习一个简单的排序算法也是非常困难的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-ratio",
   "metadata": {},
   "source": [
    "这是因为深度学习模型只是将一个向量空间映射到另一个向量空间的一系列简单、连续的几何变换。 它所能做的就是将一个数据流形 X 映射到另一个流形 Y，假设存在从 X 到 Y 的可学习的连续变换。深度学习模型可以解释为一种程序； 但是，反过来说，大多数程序不能表示为深度学习模型——对于大多数任务，要么不存在相应的合理规模的神经网络来解决该任务，要么即使存在，也可能不存在：相应的几何 转换可能太复杂了，或者可能没有合适的数据来学习它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-humanity",
   "metadata": {},
   "source": [
    "通过堆叠更多层和使用更多训练数据来扩展当前的深度学习技术只能从表面上缓解其中的一些问题。 它无法解决更基本的问题，即深度学习模型在其可以表示的内容方面受到限制，并且您可能希望学习的大多数程序无法表示为数据流形的连续几何变形。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-marsh",
   "metadata": {},
   "source": [
    "### 将机器学习模型拟人化的风险"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-gabriel",
   "metadata": {},
   "source": [
    "当代人工智能的一个真正风险是曲解深度学习模型的作用并高估了它们的能力。 人类的一个基本特征是我们的心理理论：我们倾向于将意图、信念和知识投射到我们周围的事物上。 在岩石上画一个笑脸会突然让它“快乐”——在我们的脑海中。 应用于深度学习，这意味着，例如，当我们能够在某种程度上成功地训练一个模型来生成描述图片的标题时，我们会相信该模型“理解”了图片和标题的内容 它生成。 然后，当与训练数据中存在的图像类型的任何轻微偏离都会导致模型生成完全荒谬的标题时，我们会感到惊讶（见图 14.1）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-order",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtaf1em0ibj30vy0ngjtn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-colleague",
   "metadata": {},
   "source": [
    "特别是，对抗性示例突出了这一点，这些示例是馈送到深度学习网络的样本，旨在诱使模型对它们进行错误分类。 例如，您已经知道，可以在输入空间中进行梯度上升以生成最大化某些 convnet 过滤器激活的输入——这是第 9 章介绍的过滤器可视化技术的基础，以及 第 12 章中的 DeepDream 算法。同样，通过梯度上升，您可以稍微修改图像以最大化给定类的类预测。 通过给熊猫拍张照片并添加一个长臂猿梯度，我们可以得到一个神经网络来将熊猫归类为长臂猿（见图 14.2）。 这证明了这些模型的脆弱性以及它们的输入到输出映射与我们人类感知之间的深刻差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-cabin",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtawzpkbxbj30vc0qyadv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-malta",
   "metadata": {},
   "source": [
    "简而言之，深度学习模型对它们的输入没有任何理解——至少，在人类的意义上不是。我们自己对图像、声音和语言的理解基于我们作为人类的感觉运动经验。机器学习模型无法获得此类经验，因此无法以与人类相关的方式理解它们的输入。通过注释大量训练示例以输入我们的模型，我们让它们学习几何变换，将数据映射到一组特定示例上的人类概念，但这种映射是我们脑海中原始模型的简单草图——一种是根据我们作为实体代理人的经验发展而来的。这就像镜子里的一个昏暗的图像（见图 14.3）。您创建的模型将采用任何可用的快捷方式来拟合其训练数据。例如，图像模型往往更多地依赖于局部纹理而不是对输入图像的全局理解——在具有豹子和沙发特征的数据集上训练的模型很可能将豹纹沙发分类为真正的豹子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-bennett",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtax0ths6sj314m0guach.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-recognition",
   "metadata": {},
   "source": [
    "作为机器学习从业者，请始终注意这一点，永远不要陷入相信神经网络理解它们执行的任务的陷阱——它们不会，至少不会以对我们有意义的方式。 他们接受了一项与我们想要教给他们的任务不同、范围更窄的任务：将训练输入逐点映射到训练目标。 向他们展示任何偏离他们训练数据的东西，他们会以荒谬的方式打破。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-entrance",
   "metadata": {},
   "source": [
    "### 自动机与智能代理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-praise",
   "metadata": {},
   "source": [
    "深度学习模型从输入到输出的直接几何变形与人类思考和学习的方式之间存在根本差异。 不仅仅是人类从具体的经验中自己学习，而不是被呈现给明确的训练示例。 与可微分的参数函数相比，人脑是完全不同的野兽。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-mozambique",
   "metadata": {},
   "source": [
    "让我们缩小一点并问：智能的目的是什么？为什么它首先出现？我们只能推测，但我们可以做出相当明智的推测。我们可以先看看大脑——产生智力的器官。大脑是一种进化适应——一种经过数亿年逐步发展的机制，通过自然选择指导的随机试错——极大地扩展了生物体适应环境的能力。大脑最初出现在 50 亿多年前，作为一种存储和执行行为程序的方式。 “行为程序”只是使有机体对其环境做出反应的一组指令：“如果发生这种情况，那就去做”。它们将生物体的感觉输入与其运动控制联系起来。一开始，大脑将用于硬编码行为程序（作为神经连接模式），这将允许有机体对其感官输入做出适当的反应。这就是昆虫大脑仍然工作的方式——苍蝇、蚂蚁、秀丽隐杆线虫（见图 14.4）等。因为这些程序的原始“源代码”是 DNA，它会被解码为神经连接模式，进化突然能够以一种基本上无界的方式搜索行为空间——一个重大的进化转变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-crown",
   "metadata": {},
   "source": [
    "进化是程序员，而大脑是仔细执行进化赋予他们的代码的计算机。 因为神经连接是一个非常通用的计算基础，所有具有大脑功能的物种的感觉运动空间可能会突然开始经历剧烈的扩张。 眼睛、耳朵、下颌骨、四条腿、二十四条腿——只要你有大脑，进化就会为你找出充分利用这些的行为程序。 大脑可以处理任何形式——或形式的组合——你扔给它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-milton",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtax5nyplej30r60oijww.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-chassis",
   "metadata": {},
   "source": [
    "现在，请注意，这些早期的大脑本身并不完全聪明。 它们是非常自动的：它们只会执行硬编码在生物体 DNA 中的行为程序。 它们只能被描述为智能，就像恒温器“智能”一样。 或者一个列表排序程序。 或者……一个训练有素的深度神经网络（人工类型）。 这是一个重要的区别，所以让我们仔细看看：自动机和实际的智能代理有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-commonwealth",
   "metadata": {},
   "source": [
    "### 局部泛化与极端泛化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-watch",
   "metadata": {},
   "source": [
    "17 世纪法国哲学家和科学家勒内·笛卡尔在 1637 年写了一篇启发性的评论，完美地捕捉到了这种区别——早在人工智能兴起之前，事实上，在第一台机械计算机（他的同事帕斯卡将在五年后创建）之前。 笛卡尔告诉我们，关于自动机："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-intention",
   "metadata": {},
   "source": [
    "> 即使这样的机器在某些事情上可能做的和我们一样好，或者甚至更好，但它们在其他事情上不可避免地会失败，这表明它们不是通过理解而行动，而只是通过其器官的处置。："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-bacteria",
   "metadata": {},
   "source": [
    "就是这样。 智力的特征在于理解，而理解则通过概括来证明——处理任何可能出现的新情况的能力。 你如何区分一个学生已经记住了过去三年的考题但对主题一无所知，一个真正理解了材料？ 你给他们一个全新的问题。 自动机是静态的，旨在在特定上下文中完成特定的事情——“如果这个，那么那个”——而智能代理可以即时适应新的、意想不到的情况。 当自动机接触到与其“编程”要做的事情不匹配的事情时（无论我们是在谈论人工编写的程序、进化生成的程序，还是在训练中拟合模型的隐式编程过程） 数据集），它将失败。 与此同时，像我们人类一样的智能代理将利用他们的理解来寻找前进的道路。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-faith",
   "metadata": {},
   "source": [
    "人类不仅能够像深度网络或昆虫那样将即时刺激映射到即时反应。我们维护我们当前情况、我们自己和其他人的复杂、抽象模型，并且可以使用这些模型来预测不同的可能的未来并执行长期规划。您可以将已知的概念合并在一起来表示您以前从未体验过的东西——比如想象如果中了彩票你会怎么做，或者想象如果你小心翼翼地用弹性橡胶制成的精确副本替换她的钥匙时你的朋友会如何反应。这种处理新奇和假设的能力，将我们的心智模型空间扩展到远远超出我们可以直接体验的范围——利用和抽象推理——是人类认知的决定性特征。我称之为极端概括：使用很少的数据甚至根本没有新数据来适应新的、前所未有的情况的能力。这种能力是人类和高级动物展示智能的关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-fifth",
   "metadata": {},
   "source": [
    "这与类似自动机的系统所做的形成鲜明对比。一个非常僵化的自动机根本没有任何泛化功能——它无法处理任何事先没有准确告知的事情。 Python dict 或作为硬编码 if-then-else 语句实现的基本问答程序将属于这一类。深度网络的表现稍好一些：它们可以成功地处理与它们熟悉的有点不同的输入——这正是它们有用的原因。我们第 8 章中的猫对狗模型可以对它以前从未见过的猫或狗图片进行分类，只要它们与训练的图片足够接近即可。然而，深度网络仅限于我所说的局部泛化（见图 14.5）：当输入开始偏离网络在训练时看到的内容时，深度网络执行的从输入到输出的映射很快就失去意义。深度网络只能泛化到已知的未知数——模型开发过程中预期的变异因素，以及训练数据中广泛存在的特征，例如不同的相机角度或宠物照片的照明条件。这是因为深度网络通过流形上的插值进行泛化（记住第 5 章）：输入空间中的任何变化因素都需要被它们学习的流形捕获。这就是为什么基本数据增强对改进深度网络泛化如此有帮助的原因。与人类不同的是，这些模型无法在数据很少或没有可用数据的情况下即兴发挥（例如中彩票或被递上橡胶钥匙），这些情况仅与过去的情况具有抽象的共性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-bedroom",
   "metadata": {},
   "source": [
    "例如，考虑学习适当的发射参数以使火箭降落在月球上的问题。如果您使用深度网络完成这项任务并使用监督学习或强化学习对其进行训练，则您必须为其提供数万次甚至数百万次启动试验：您需要将其暴露于输入的密集采样中空间，以便它学习从输入空间到输出空间的可靠映射。相比之下，作为人类，我们可以利用我们的抽象能力来提出物理模型——火箭科学——并推导出一个精确的解决方案，在一次或几次试验中将火箭降落在月球上。类似地，如果你开发了一个控制人体的深度网络，并且你想让它学会在不被汽车撞到的情况下安全地在城市中穿行，那么网络将不得不在各种情况下死亡数千次，直到它可以推断出汽车是危险，并制定适当的回避行为。进入一个新城市，网络将不得不重新学习它所知道的大部分内容。另一方面，人类能够学习安全的行为而不必死一次——再次感谢我们对新情况的抽象建模能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-branch",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtaxcqenbbj315s0na40p.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-dimension",
   "metadata": {},
   "source": [
    "### 智能的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-extent",
   "metadata": {},
   "source": [
    "高度适应性智能代理和刚性自动机之间的这种区别将我们带回了大脑进化。 为什么大脑——最初只是自然进化发展行为自动机的媒介——最终变得智能？ 就像每一个重要的进化里程碑一样，它的发生是因为自然选择的限制促使它发生。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-character",
   "metadata": {},
   "source": [
    "大脑负责行为的产生。如果有机体必须面对的情况大多是静态的并且是预先知道的，那么行为生成将是一个简单的问题：进化只是通过随机试错找出正确的行为，并将它们硬编码到有机体的 DNA 中。大脑进化的第一阶段——大脑作为自动机——已经是最佳的。然而，至关重要的是，随着生物体的复杂性——以及随之而来的环境复杂性——不断增加，动物必须应对的情况变得更加动态和不可预测。如果你仔细观察，你生命中的一天不同于你经历过的任何一天，也不同于你的任何进化祖先经历过的任何一天。您需要能够不断面对未知和令人惊讶的情况。进化无法找到并将您作为 DNA 硬编码的行为序列，以便您从几个小时前醒来后成功地度过一天。它必须每天动态生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-finger",
   "metadata": {},
   "source": [
    "大脑，作为一个很好的行为生成引擎，只是适应了这种需求。 它针对适应性和通用性本身进行了优化，而不仅仅是针对一组固定情况进行优化。 这种转变可能在整个进化史上发生过多次，导致在非常遥远的进化分支——猿、章鱼、乌鸦等——中产生了高度智能的动物。 智能是应对复杂、动态生态系统所带来挑战的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-disclosure",
   "metadata": {},
   "source": [
    "这就是智能的本质：它能够有效地利用您所掌握的信息，以便在面对不确定、不断变化的未来时产生成功的行为。 笛卡尔所说的“理解”是这种非凡能力的关键：挖掘你过去的经验来开发模块化、可重用的抽象的能力，这些抽象可以快速重新用于处理新情况，并实现极端泛化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-democracy",
   "metadata": {},
   "source": [
    "### 攀登泛化的范围"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-massachusetts",
   "metadata": {},
   "source": [
    "作为粗略的漫画，您可以将生物智能的进化历史概括为泛化范围的缓慢攀升。 它始于只能执行局部泛化的类似自动机的大脑。 随着时间的推移，进化开始产生能够进行更广泛泛化的生物体，它们可以在越来越复杂和多变的环境中茁壮成长。 最终，在过去的几百万年里——从进化的角度来说，是一个瞬间——某些人类物种开始趋向于实现能够极端概括的生物智能，促使人类世的开始并永远改变地球上生命的历史。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-paraguay",
   "metadata": {},
   "source": [
    "人工智能在过去 70 年的进步与这种演变有着惊人的相似之处。 早期的人工智能系统是纯自动机，比如 1960 年代的 ELIZA 聊天程序，或 SHRDLU：1970 年的人工智能，能够通过自然语言命令操作简单的对象。 在 1990 年代和 2000 年代，我们看到了能够进行局部泛化的机器学习系统的兴起，它可以处理一定程度的不确定性和新颖性。 在 2010 年代，深度学习使工程师能够利用更大的数据集和更具表现力的模型，进一步扩展了这些系统的局部泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-interaction",
   "metadata": {},
   "source": [
    "今天，我们可能正处于下一个进化步骤的风口浪尖上。 人们对能够实现广泛泛化的系统越来越感兴趣，我将其定义为在单一广泛的任务领域内处理未知未知数的能力（包括系统未经训练处理的情况及其创建者无法预料的情况）。 例如，一辆能够安全处理任何情况的自动驾驶汽车，或者一个可以通过“Woz 智力测试”的家用机器人——随机进入厨房并煮一杯咖啡。 通过将深度学习和精心制作的世界抽象模型相结合，我们已经在朝着这些目标取得明显进展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-merchant",
   "metadata": {},
   "source": [
    "然而，目前，人工智能仍然仅限于认知自动化：“人工智能”中的“智能”标签是一个类别错误。 将我们的领域称为“人工智能”会更准确，“认知自动化”和“人工智能”是其中两个几乎独立的子领域。 在这个细分领域，“人工智能”将是一片绿地，几乎所有东西都有待发现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-gallery",
   "metadata": {},
   "source": [
    "现在，我并不是要贬低深度学习的成就。 认知自动化非常有用，深度学习模型能够将任务从暴露到数据中自动化的方式代表了认知自动化的一个特别强大的方法，比显式编程更实用和通用。 做好这件事基本上可以改变每个行业的游戏规则。 但这离人类（或动物）智能还有很长的路要走。 到目前为止，我们的模型只能执行局部泛化：它们通过从 X 到 Y 数据点的密集采样中学习到的平滑几何变换将空间 X 映射到空间 Y，并且空间 X 或 Y 内的任何中断都会使该映射无效。 它们只能泛化到与过去数据保持相似的新情况——而人类认知能够极端泛化，快速适应全新的情况并为未来的长期情况做规划。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-visibility",
   "metadata": {},
   "source": [
    "## 为 AI 的通用性设置课程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-works",
   "metadata": {},
   "source": [
    "为了消除我们讨论过的一些限制并创建可以与人脑竞争的人工智能，我们需要从简单的输入到输出映射转向推理和抽象。 在接下来的几节中，我们将看看未来的道路可能是什么样子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-charter",
   "metadata": {},
   "source": [
    "### 关于设定正确目标的重要性：捷径规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-works",
   "metadata": {},
   "source": [
    "生物智能是对大自然提出的问题的回答。 同样，如果我们想开发真正的人工智能，首先，我们需要提出正确的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-steering",
   "metadata": {},
   "source": [
    "您在系统设计中经常看到的一个效果是捷径规则：如果您专注于优化一个成功指标，您将实现目标，但代价是系统中未包含在您的成功指标中的所有内容。 您最终会采取所有可用的捷径来实现目标。 你的创作是由你给自己的激励塑造的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-immigration",
   "metadata": {},
   "source": [
    "您经常在机器学习竞赛中看到这一点。 2009 年，Netflix 发起了一项挑战，承诺向在电影推荐任务中获得最高分的团队提供 100 万美元的奖金。 它最终从未使用获胜团队创建的系统，因为它太复杂且计算密集。 获胜者仅针对预测准确性进行了优化——他们被激励实现的目标——以牺牲系统的所有其他理想特性为代价：推理成本、可维护性、可解释性。 捷径规则在大多数 Kaggle 比赛中也适用——Kaggle 获胜者制作的模型很少（如果有的话）用于生产。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-forwarding",
   "metadata": {},
   "source": [
    "在过去的几十年里，捷径规则在人工智能中无处不在。 在 1970 年代，心理学家和计算机科学先驱艾伦纽厄尔担心他的领域在正确的认知理论方面没有取得任何有意义的进展，为人工智能提出了一个新的宏伟目标：下棋。 其基本原理是，人类下棋似乎涉及——甚至可能需要——诸如感知、推理和分析、记忆和书本学习等能力。 当然，如果我们能建造一个国际象棋机器，它也必须具有这些属性。 对？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-geometry",
   "metadata": {},
   "source": [
    "24 年后，梦想成真：1997 年，IBM 的 DeepBlue 击败了世界上最好的国际象棋选手 Gary Kasparov。 然后，研究人员不得不面对这样一个事实，即创建国际象棋冠军 AI 并没有教会他们关于人类智能的知识。 DeepBlue 核心的 A-star 算法不是人脑模型，不能推广到类似棋盘游戏以外的任务。 事实证明，建立一个只会下棋的人工智能比建立一个人工大脑更容易——所以这就是研究人员采取的捷径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-registration",
   "metadata": {},
   "source": [
    "迄今为止，AI 领域的驱动成功指标一直是解决特定任务，从国际象棋到围棋，从 MNIST 分类到 ImageNet，从 Atari Arcade 游戏到星际争霸和 DotA 2。因此，该领域的历史已经被定义 通过一系列“成功”，我们找到了如何在没有任何智能的情况下解决这些任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-wiring",
   "metadata": {},
   "source": [
    "如果这听起来令人惊讶的话，请记住，类人智能的特征不在于任何特定任务的技能——而是适应新事物、有效获取新技能和掌握前所未见的能力任务。通过修复任务，您可以对需要完成的工作提供任意精确的描述——通过硬编码人类提供的知识，或通过提供大量数据。工程师只需添加数据或添加硬编码知识，就可以为他们的 AI“购买”更多技能，而无需增加 AI 的泛化能力（见图 14.6）。如果你有近乎无限的训练数据，即使是像最近邻搜索这样非常粗糙的算法也能以超人的技巧玩电子游戏。同样，如果您有几乎无限量的人工编写的 if-then-else 语句。那就是......直到你对游戏规则做一个小的改变——人类可以立即适应的那种——这将需要重新训练或从头开始重建非智能系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-profile",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtayfyrykvj31c60m0q5e.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-monthly",
   "metadata": {},
   "source": [
    "简而言之，通过固定任务，您就不需要处理不确定性和新颖性，而且由于智能的本质是处理不确定性和新颖性的能力，因此您可以有效地消除对智能的需求。 而且因为找到一个非智能的人总是更容易\n",
    "解决特定任务而不是解决一般的智力问题，这是你 100% 时间都会走的捷径。 人类可以利用他们的通用智能来获得任何新任务的技能，但反过来，从特定任务的技能集合到通用智能是没有途径的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-conjunction",
   "metadata": {},
   "source": [
    "### 新目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-measurement",
   "metadata": {},
   "source": [
    "为了使人工智能真正具有智能，并赋予它处理现实世界令人难以置信的可变性和不断变化的性质的能力，首先，我们需要摆脱寻求实现特定任务的技能，而是开始瞄准泛化 权力本身。 我们需要新的进展指标来帮助我们开发越来越智能的系统。 指标将指向正确的方向，并为我们提供可操作的反馈信号。 只要我们将目标设定为“创建一个解决任务 X 的模型”，捷径规则就会适用，我们最终会得到一个执行 X 的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-catch",
   "metadata": {},
   "source": [
    "在我看来，智力可以精确地量化为一个效率比：你所掌握的有关世界的相关信息量（可以是过去的经验、先验知识、未来的操作领域，或先天）与你的、集合之间的转换比您将能够产生适当行为的新情况（您可以将其视为您的）。更智能的代理将能够使用更少的过去经验来处理更广泛的未来任务和技能组合情况。要衡量这样的比率，您只需要修复系统可用的信息、它的经验和先验知识——并衡量它在一组已知与系统已访问的内容大不相同的参考情况或任务上的性能到。试图最大化这个比率应该会引导你走向智慧。至关重要的是，为了避免作弊，您将需要确保仅在未经过编程或培训处理的任务上测试系统——事实上，您需要系统创建者无法预料的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-gamma",
   "metadata": {},
   "source": [
    "在 2018 年和 2019 年，我开发了一个名为抽象与推理语料库 (ARC) 的基准数据集：旨在捕捉智能的这种定义。 ARC 旨在让机器和人类都可以使用，它看起来与人类 IQ 测试非常相似，例如 Raven 的渐进矩阵。 在测试时，您会看到一系列“任务”。 每个任务都通过三个或四个“示例”来解释，这些“示例”采用输入网格和相应的输出网格的形式（见图 14.7）。 然后，您将获得一个全新的输入网格，在继续下一个任务之前，您将有 3 次尝试生成正确的输出网格。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-massage",
   "metadata": {},
   "source": [
    "![](https://tva1.sinaimg.cn/large/008i3skNgy1gtayjun4nbj31b60n4wjv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-affairs",
   "metadata": {},
   "source": [
    "与 IQ 测试相比，ARC 有两个独特之处。 首先，ARC 旨在衡量泛化能力，只在您以前从未见过的任务上测试您。 这意味着 ARC 是一款您无法练习的游戏，至少在理论上是这样：您将接受测试的任务将具有自己独特的逻辑，您必须即时理解这些逻辑。 您不能只记住过去任务中的特定策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-arlington",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
